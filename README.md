# SmartLogiTrack ETA Control Tower

Bienvenue dans la documentation technique de **SmartLogiTrack**, une tour de contr√¥le logistique avanc√©e capable de pr√©dire les temps d'arriv√©e (ETA) et d'analyser la performance des trajets en temps r√©el.

Ce projet d√©montre une architecture **Data Engineering & AI** compl√®te, allant du nettoyage de donn√©es massives √† leur exposition via une API s√©curis√©e.

---

## 1. Architecture ETL (Extract, Transform, Load)

L'ingestion et le traitement des donn√©es suivent l'architecture **Medallion** (Bronze ‚û°Ô∏è Silver ‚û°Ô∏è Gold) pour garantir la qualit√© des donn√©es.

### üîπ Nettoyage avec Apache Spark (Couche Silver)
Nous utilisons **PySpark** pour traiter les donn√©es brutes (Bronze) et produire une table propre (Silver) nomm√©e `silver_taxi_trips`.

**Pourquoi Spark ?**  
Pour sa capacit√© √† traiter des millions de lignes (Big Data) en parall√®le, l√† o√π Pandas serait limit√© par la RAM.

**Les √©tapes de nettoyage cl√©s :**
1.  **Filtrage des anomalies** : Suppression des trajets avec des dur√©es n√©gatives ou des distances nulles.
    ```python
    df_clean = df.filter((col("trip_distance") > 0) & (col("duration_minutes") > 0))
    ```
2.  **Standardisation des types** : Conversion des timestamps et typage strict des colonnes num√©riques.
3.  **Enrichissement** : Ajout de colonnes d√©riv√©es comme `day_of_week` ou `pickup_hour` pour faciliter l'analyse en aval.

---

## 2. Service de Pr√©diction (IA & Asynchrone)

Le c≈ìur de l'intelligence r√©side dans notre mod√®le de Machine Learning (`model_eta.pkl`), int√©gr√© dans une API **FastAPI**.

### Integration du Mod√®le .pkl
Le mod√®le Random Forest est entra√Æn√© s√©par√©ment et s√©rialis√© avec `joblib`.

- **Chargement au d√©marrage** : Le mod√®le est charg√© une seule fois en m√©moire √† l'instanciation de `PredictionService` pour √©viter de le recharger √† chaque requ√™te .
- **Inf√©rence** : L'API re√ßoit les caract√©ristiques du trajet (distance, heure...) et interroge le mod√®le.
- **Monitoring (Logging Asynchrone)** : 
    Chaque pr√©diction est sauvegard√©e en base de donn√©es (`eta_predictions`) pour surveiller la performance du mod√®le dans le temps (Data Drift).
    > **Note** : L'enregistrement en base est encadr√© par un `try/except` et un `rollback` pour s'assurer que si la base de donn√©es flanche, l'utilisateur re√ßoit quand m√™me sa pr√©diction.

---

## 3. S√©curit√© (JWT - JSON Web Tokens)

L'API n'est pas ouverte √† tous. Nous s√©curisons l'acc√®s aux donn√©es sensibles via le standard **OAuth2 avec JWT**.

**Le flux d'authentification :**
1.  L'utilisateur envoie `username` + `password` √† l'endpoint `/token`.
2.  Le serveur v√©rifie les identifiants et g√©n√®re un **Token sign√©** (chiffr√© avec une cl√© secr√®te `SECRET_KEY`).
3.  Ce token contient l'identit√© de l'utilisateur et une date d'expiration.
4.  Pour acc√©der √† `/predict` ou `/analytics/*`, l'utilisateur doit envoyer ce token dans le header `Authorization: Bearer <token>`.

**Avantage** : Le serveur est "stateless". Il n'a pas besoin de garder une session utilisateur en m√©moire ; la validit√© du token suffit.

---

## 4. Analytics & Performance SQL (CTEs)

Pour les tableaux de bord, la performance est critique. Nous ne faisons **aucun calcul c√¥t√© Python**.

### Common Table Expressions (CTE)
Au lieu de charger 1 million de lignes dans Python pour calculer une moyenne, nous envoyons une requ√™te SQL optimis√©e qui d√©l√®gue le travail au moteur de base de donn√©es (PostgreSQL).

**Exemple utilis√© dans `AnalyticsService` :**
```sql
WITH HourlyStats AS (
    -- Pr√©-agr√©gation des donn√©es par heure
    SELECT pickup_hour, AVG(duration_minutes) as mean_duration
    FROM silver_taxi_trips
    GROUP BY pickup_hour
)
-- S√©lection finale format√©e
SELECT pickup_hour, ROUND(mean_duration, 2)
FROM HourlyStats
ORDER BY pickup_hour;
```

**Pourquoi c'est mieux ?**
- **Moins de transfert r√©seau** : Seuls 24 lignes (pour 24h) transitent vers l'API, au lieu de millions.
- **Vitesse** : PostgreSQL est ultra-optimis√© pour les agr√©gations (`GROUP BY`, `AVG`).

---

## Guide de D√©marrage

### Pr√©-requis
- Python 3.10+
- PostgreSQL

### Installation
```bash
pip install -r requirements.txt
```

### Lancement du Serveur
```bash
uvicorn app.main:app --reload
```

### Tests
Lancer la suite de tests automatis√©s :
```bash
pytest -v
```